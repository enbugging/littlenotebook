@article{Bombari2025a,
abstract = {Differentially private (DP) linear regression has received significant attention in the recent theoretical literature, with several works aimed at obtaining improved error rates. A common approach is to set the clipping constant much larger than the expected norm of the per-sample gradients. While simplifying the analysis, this is however in sharp contrast with what empirical evidence suggests to optimize performance. Our work bridges this gap between theory and practice: we provide sharper rates for DP stochastic gradient descent (DP-SGD) by crucially operating in a regime where clipping happens frequently. Specifically, we consider the setting where the data is multivariate Gaussian, the number of training samples $n$ is proportional to the input dimension $d$, and the algorithm guarantees constant-order zero concentrated DP. Our method relies on establishing a deterministic equivalent for the trajectory of DP-SGD in terms of a family of ordinary differential equations (ODEs). As a consequence, the risk of DP-SGD is bounded between two ODEs, with upper and lower bounds matching for isotropic data. By studying these ODEs when $n / d$ is large enough, we demonstrate the optimality of aggressive clipping, and we uncover the benefits of decaying learning rate and private noise scheduling.},
archivePrefix = {arXiv},
arxivId = {2505.16329},
author = {Bombari, Simone and Seroussi, Inbar and Mondelli, Marco},
eprint = {2505.16329},
title = {{Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping}},
url = {http://arxiv.org/abs/2505.16329},
year = {2025}
}
@article{Vershynin2018,
abstract = {An introduction with applications in data science},
author = {Vershynin, Roman},
doi = {10.1017/9781108231596},
journal = {High-Dimensional Probability},
title = {{High-Dimensional Probability}},
year = {2018}
}
@article{Sonthalia2025,
abstract = {This paper investigates low-rank structure in the gradients of the training loss for two-layer neural networks while relaxing the usual isotropy assumptions on the training data and parameters. We consider a spiked data model in which the bulk can be anisotropic and ill-conditioned, we do not require independent data and weight matrices and we also analyze both the mean-field and neural-tangent-kernel scalings. We show that the gradient with respect to the input weights is approximately low rank and is dominated by two rank-one terms: one aligned with the bulk data–residue, and another aligned with the rank one spike in the input data. We characterize how properties of the training data, the scaling regime and the activation function govern the balance between these two components. Additionally, we also demonstrate that standard regularizers, such as weight decay, input noise and Jacobian penalties, also selectively modulate these components. Experiments on synthetic and real data corroborate our theoretical predictions.},
author = {Sonthalia, Rishi and Murray, Michael and Montufar, Guido},
journal = {ICML 2025 Workshop HiLD},
keywords = {One step gradient,high dimensional probability,low-rank gradients,regularization,spiked covariance,two-layer neural networks},
title = {{Low Rank Gradients and Where to Find Them}},
year = {2025}
}
@article{Tropp2015,
abstract = {Random matrices now play a role in many areas of theoretical, applied, and computational mathematics. Therefore, it is desirable to have tools for studying random matrices that are flexible, easy to use, and powerful. Over the last fifteen years, researchers have developed a remarkable family of results, called matrix concentration inequalities, that achieve all of these goals. This monograph offers an invitation to the field of matrix concentration inequalities. It begins with some history of random matrix theory; it describes a flexible model for random matrices that is suitable for many problems; and it discusses the most important matrix concentration results. To demonstrate the value of these techniques, the presentation includes examples drawn from statistics, machine learning, optimization, combinatorics, algorithms, scientific computing, and beyond.},
archivePrefix = {arXiv},
arxivId = {1501.01571},
author = {Tropp, Joel A.},
doi = {10.1561/2200000048},
eprint = {1501.01571},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
number = {1-2},
pages = {1--230},
title = {{An introduction to matrix concentration inequalities}},
volume = {8},
year = {2015}
}
@article{Yu2021,
abstract = {The privacy leakage of the model about the training data can be bounded in the differential privacy mechanism. However, for meaningful privacy parameters, a differentially private model degrades the utility drastically when the model comprises a large number of trainable parameters. In this paper, we propose an algorithm Gradient Embedding Perturbation (GEP) towards training differentially private deep models with decent accuracy. Specifically, in each gradient descent step, GEP first projects individual private gradient into a non-sensitive anchor subspace, producing a low-dimensional gradient embedding and a small-norm residual gradient. Then, GEP perturbs the low-dimensional embedding and the residual gradient separately according to the privacy budget. Such a decomposition permits a small perturbation variance, which greatly helps to break the dimensional barrier of private learning. With GEP, we achieve decent accuracy with reasonable computational cost and modest privacy guarantee for deep models. Especially, with privacy bound $\epsilon$ = 8, we achieve 74.9% test accuracy on CIFAR10 and 95.1% test accuracy on SVHN, significantly improving over existing results.},
archivePrefix = {arXiv},
arxivId = {2102.12677},
author = {Yu, Da and Zhang, Huishuai and Chen, Wei and Liu, Tie Yan},
eprint = {2102.12677},
journal = {ICLR 2021 - 9th International Conference on Learning Representations},
title = {{Do Not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning}},
year = {2021}
}
@article{Mehta2022,
abstract = {Differential Privacy (DP) provides a formal framework for training machine learning models with individual example level privacy. In the field of deep learning, Differentially Private Stochastic Gradient Descent (DP-SGD) has emerged as a popular private training algorithm. Unfortunately, the computational cost of training large-scale models with DP-SGD is substantially higher than non-private training. This is further exacerbated by the fact that increasing the number of parameters leads to larger degradation in utility with DP. In this work, we zoom in on the ImageNet dataset and demonstrate that, similar to the non-private case, pre-training over-parameterized models on a large public dataset can lead to substantial gains when the model is finetuned privately. Moreover, by systematically comparing private and non-private models across a range of large batch sizes, we find that similar to non-private setting, choice of optimizer can further improve performance substantially with DP. By using LAMB optimizer with DP-SGD we saw improvement of up to 20$\%$ points (absolute). Finally, we show that finetuning just the last layer for a \emph{single step} in the full batch setting, combined with extremely small-scale (near-zero) initialization leads to both SOTA results of 81.7 $\%$ under a wide privacy budget range of $\epsilon\in [4, 10]$ and $\delta$ = $10^{-6}$ while minimizing the computational overhead substantially.},
archivePrefix = {arXiv},
arxivId = {2205.02973},
author = {Mehta, Harsh and Thakurta, Abhradeep and Kurakin, Alexey and Cutkosky, Ashok},
eprint = {2205.02973},
month = {may},
title = {{Large Scale Transfer Learning for Differentially Private Image Classification}},
url = {http://arxiv.org/abs/2205.02973},
year = {2022}
}
@article{Kairouz2020,
abstract = {We revisit the problem of empirical risk minimziation (ERM) with differential privacy. We show that noisy AdaGrad, given appropriate knowledge and conditions on the subspace from which gradients can be drawn, achieves a regret comparable to traditional AdaGrad plus a well-controlled term due to noise. We show a convergence rate of $O(\text{Tr}(G_T)/T)$, where $G_T$ captures the geometry of the gradient subspace. Since $\text{Tr}(G_T)=O(\sqrt{T})$ we can obtain faster rates for convex and Lipschitz functions, compared to the $O(1/\sqrt{T})$ rate achieved by known versions of noisy (stochastic) gradient descent with comparable noise variance. In particular, we show that if the gradients lie in a known constant rank subspace, and assuming algorithmic access to an envelope which bounds decaying sensitivity, one can achieve faster convergence to an excess empirical risk of $\tilde O(1/\epsilon n)$, where $\epsilon$ is the privacy budget and $n$ the number of samples. Letting $p$ be the problem dimension, this result implies that, by running noisy Adagrad, we can bypass the DP-SGD bound $\tilde O(\sqrt{p}/\epsilon n)$ in $T=(\epsilon n)^{2/(1+2\alpha)}$ iterations, where $\alpha \geq 0$ is a parameter controlling gradient norm decay, instead of the rate achieved by SGD of $T=\epsilon^2n^2$. Our results operate with general convex functions in both constrained and unconstrained minimization. Along the way, we do a perturbation analysis of noisy AdaGrad of independent interest. Our utility guarantee for the private ERM problem follows as a corollary to the regret guarantee of noisy AdaGrad.},
archivePrefix = {arXiv},
arxivId = {2008.06570},
author = {Kairouz, Peter and Ribero, M{\'{o}}nica and Rush, Keith and Thakurta, Abhradeep},
eprint = {2008.06570},
title = {{Fast Dimension Independent Private AdaGrad on Publicly Estimated Subspaces}},
url = {http://arxiv.org/abs/2008.06570},
year = {2020}
}
@article{Zhou2021,
abstract = {Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension p, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where p ≫ n, the number of training samples. Existing lower bounds on private ERM show that such dependence on p is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks-that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in p. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\epsilon$).},
archivePrefix = {arXiv},
arxivId = {2007.03813},
author = {Zhou, Yingxue and Wu, Zhiwei Steven and Banerjee, Arindam},
eprint = {2007.03813},
journal = {ICLR 2021 - 9th International Conference on Learning Representations},
title = {{Bypassing the Ambient Dimension: Private Sgd With Gradient Subspace Identification}},
year = {2021}
}
@article{De2022a,
abstract = {Differential Privacy (DP) provides a formal privacy guarantee preventing adversaries with access to a machine learning model from extracting information about individual training points. Differentially Private Stochastic Gradient Descent (DP-SGD), the most popular DP training method for deep learning, realizes this protection by injecting noise during training. However previous works have found that DP-SGD often leads to a significant degradation in performance on standard image classification benchmarks. Furthermore, some authors have postulated that DP-SGD inherently performs poorly on large models, since the norm of the noise required to preserve privacy is proportional to the model dimension. In contrast, we demonstrate that DP-SGD on over-parameterized models can perform significantly better than previously thought. Combining careful hyper-parameter tuning with simple techniques to ensure signal propagation and improve the convergence rate, we obtain a new SOTA without extra data on CIFAR-10 of 81.4% under (8, 10^{-5})-DP using a 40-layer Wide-ResNet, improving over the previous SOTA of 71.7%. When fine-tuning a pre-trained NFNet-F3, we achieve a remarkable 83.8% top-1 accuracy on ImageNet under (0.5, 8*10^{-7})-DP. Additionally, we also achieve 86.7% top-1 accuracy under (8, 8 \cdot 10^{-7})-DP, which is just 4.3% below the current non-private SOTA for this task. We believe our results are a significant step towards closing the accuracy gap between private and non-private image classification.},
archivePrefix = {arXiv},
arxivId = {2204.13650},
author = {De, Soham and Berrada, Leonard and Hayes, Jamie and Smith, Samuel L. and Balle, Borja},
eprint = {2204.13650},
title = {{Unlocking High-Accuracy Differentially Private Image Classification through Scale}},
url = {http://arxiv.org/abs/2204.13650},
year = {2022}
}
@article{Hitaj2017,
abstract = {Deep Learning has recently become hugely popular in machine learning for its ability to solve end-to-end learning systems, in which the features and the classifiers are learned simultaneously, providing significant improvements in classification accuracy in the presence of highly-structured and large databases. Its success is due to a combination of recent algorithmic breakthroughs, increasingly powerful computers, and access to significant amounts of data. Researchers have also considered privacy implications of deep learning. Models are typically trained in a centralized manner with all the data being processed by the same training algorithm. If the data is a collection of users' private data, including habits, personal pictures, geographical positions, interests, and more, the centralized server will have access to sensitive information that could potentially be mishandled. To tackle this problem, collaborative deep learning models have recently been proposed where parties locally train their deep learning structures and only share a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy (DP) to make information extraction even more challenging, as proposed by Shokri and Shmatikov at CCS'15. Unfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed, federated, or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversarial Network (GAN) that generates prototypical samples of the targeted training set that was meant to be private (the samples generated by the GAN are intended to come from the same distribution as the training data). Interestingly, we show that record-level differential privacy applied to the shared parameters of the model, as suggested in previous work, is ineffective (i.e., record-level DP is not designed to address our attack).},
archivePrefix = {arXiv},
arxivId = {1702.07464},
author = {Hitaj, Briland and Ateniese, Giuseppe and Perez-Cruz, Fernando},
doi = {10.1145/3133956.3134012},
eprint = {1702.07464},
isbn = {9781450349468},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
keywords = {Collaborative Learning,Deep Learning,Privacy,Security},
pages = {603--618},
title = {{Deep Models under the GAN: Information leakage from collaborative deep learning}},
year = {2017}
}
@article{Yu2021a,
abstract = {The privacy leakage of the model about the training data can be bounded in the differential privacy mechanism. However, for meaningful privacy parameters, a differentially private model degrades the utility drastically when the model comprises a large number of trainable parameters. In this paper, we propose an algorithm Gradient Embedding Perturbation (GEP) towards training differentially private deep models with decent accuracy. Specifically, in each gradient descent step, GEP first projects individual private gradient into a non-sensitive anchor subspace, producing a low-dimensional gradient embedding and a small-norm residual gradient. Then, GEP perturbs the low-dimensional embedding and the residual gradient separately according to the privacy budget. Such a decomposition permits a small perturbation variance, which greatly helps to break the dimensional barrier of private learning. With GEP, we achieve decent accuracy with reasonable computational cost and modest privacy guarantee for deep models. Especially, with privacy bound $\epsilon$ = 8, we achieve 74.9% test accuracy on CIFAR10 and 95.1% test accuracy on SVHN, significantly improving over existing results.},
archivePrefix = {arXiv},
arxivId = {2102.12677},
author = {Yu, Da and Zhang, Huishuai and Chen, Wei and Liu, Tie Yan},
eprint = {2102.12677},
journal = {ICLR 2021 - 9th International Conference on Learning Representations},
title = {{Do Not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning}},
year = {2021}
}
@article{Bruna2025,
abstract = {We review the literature on algorithms for estimating the index space in a multi-index model. The primary focus is on computationally efficient (polynomial-time) algorithms in Gaussian space, the assumptions under which consistency is guaranteed by these methods, and their sample complexity. In many cases, a gap is observed between the sample complexity of the best known computationally efficient methods and the information-theoretical minimum. We also review algorithms based on estimating the span of gradients using nonparametric methods, and algorithms based on fitting neural networks using gradient descent},
archivePrefix = {arXiv},
arxivId = {2504.05426},
author = {Bruna, Joan and Hsu, Daniel},
eprint = {2504.05426},
month = {jun},
title = {{Survey on Algorithms for multi-index models}},
url = {http://arxiv.org/abs/2504.05426},
year = {2025}
}
@article{Li2022,
abstract = {Large pretrained models can be fine-tuned with differential privacy to achieve performance approaching that of non-private models. A common theme in these results is the surprising observation that high-dimensional models can achieve favorable privacy-utility trade-offs. This seemingly contradicts known results on the model-size dependence of differentially private convex learning and raises the following research question: When does the performance of differentially private learning not degrade with increasing model size? We identify that the magnitudes of gradients projected onto subspaces is a key factor that determines performance. To precisely characterize this for private convex learning, we introduce a condition on the objective that we term restricted Lipschitz continuity and derive improved bounds for the excess empirical and population risks that are dimension-independent under additional conditions. We empirically show that in private fine-tuning of large language models, gradients obtained during fine-tuning are mostly controlled by a few principal components. This behavior is similar to conditions under which we obtain dimension-independent bounds in convex settings. Our theoretical and empirical results together provide a possible explanation for the recent success of large-scale private fine-tuning. Code to reproduce our results can be found at https://github.com/lxuechen/private-transformers/tree/main/examples/classification/spectral_analysis.},
archivePrefix = {arXiv},
arxivId = {2207.00160},
author = {Li, Xuechen and Liu, Daogao and Hashimoto, Tatsunori and Inan, Huseyin A. and Kulkarni, Janardhan and Lee, Yin Tat and Thakurta, Abhradeep Guha},
eprint = {2207.00160},
isbn = {9781713871088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{When Does Differentially Private Learning Not Suffer in High Dimensions?}},
volume = {35},
year = {2022}
}
@article{Oko2024,
abstract = {We study the computational and sample complexity of learning a target function f∗ : ℝd → ℝ with additive structure, that is, f∗(x) = 1√M-Mm=1 fm(〈x, vm〉), where f1, f2, ..., fM : ℝ → ℝ are nonlinear link functions of single-index models (ridge functions) with diverse and near-orthogonal index features {vm}Mm=1, and the number of additive tasks M grows with the dimensionality M ≍ d$\gamma$ for $\gamma$ ≥ 0. This problem setting is motivated by the classical additive model literature, the recent representation learning theory of two-layer neural network, and large-scale pretraining where the model simultaneously acquires a large number of “skills” that are often localized in distinct parts of the trained network. We prove that a large subset of polynomial f∗ can be efficiently learned by gradient descent training of a two-layer neural network, with a polynomial statistical and computational complexity that depends on the number of tasks M and the information exponent of fm, despite the unknown link function and M growing with the dimensionality. We complement this learnability guarantee with computational hardness result by establishing statistical query (SQ) lower bounds for both the correlational SQ and full SQ algorithms.},
author = {Oko, Kazusato and Song, Yujin and Suzuki, Taiji and Wu, Denny},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
pages = {4009--4081},
title = {{Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations}},
volume = {247},
year = {2024}
}
@article{Shokri2015,
abstract = {Deep learning based on artificial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneficiaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training. Massive data collection required for deep learning presents obvious privacy issues. Users' personal, highly sensitive data such as photos and voice recordings is kept indefinitely by the companies that collect it. Users can neither delete it, nor restrict the purposes for which it is used. Furthermore, centrally kept data is subject to legal subpoenas and extra-judicial surveillance. Many data owners-for example, medical institutions that may want to apply deep learning methods to clinical records-are prevented by privacy and confidentiality concerns from sharing the data and thus benefitting from large-scale deep learning. In this paper, we design, implement, and evaluate a practical system that enables multiple parties to jointly learn an accurate neuralnetwork model for a given objective without sharing their input datasets. We exploit the fact that the optimization algorithms used in modern deep learning, namely, those based on stochastic gradient descent, can be parallelized and executed asynchronously. Our system lets participants train independently on their own datasets and selectively share small subsets of their models' key parameters during training. This offers an attractive point in the utility/privacy tradeoff space: participants preserve the privacy of their respective data while still benefitting from other participants' models and thus boosting their learning accuracy beyond what is achievable solely on their own inputs. We demonstrate the accuracy of our privacypreserving deep learning on benchmark datasets.},
author = {Shokri, Reza and Shmatikov, Vitaly},
doi = {10.1145/2810103.2813687},
isbn = {9781450338325},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
keywords = {Deep learning,Gradient Descent,Neural networks,Privacy},
pages = {1310--1321},
title = {{Privacy-preserving deep learning}},
volume = {2015-Octob},
year = {2015}
}
@article{Bietti2023,
abstract = {We study gradient flow on the multi-index regression problem for high-dimensional Gaussian data. Multi-index functions consist of a composition of an unknown low-rank linear projection and an arbitrary unknown, low-dimensional link function. As such, they constitute a natural template for feature learning in neural networks. We consider a two-timescale algorithm, whereby the low-dimensional link function is learnt with a non-parametric model infinitely faster than the subspace parametrizing the low-rank projection. By appropriately exploiting the matrix semigroup structure arising over the subspace correlation matrices, we establish global convergence of the resulting Grassmannian population gradient flow dynamics, and provide a quantitative description of its associated `saddle-to-saddle' dynamics. Notably, the timescales associated with each saddle can be explicitly characterized in terms of an appropriate Hermite decomposition of the target link function. In contrast with these positive results, we also show that the related \emph{planted} problem, where the link function is known and fixed, in fact has a rough optimization landscape, in which gradient flow dynamics might get trapped with high probability.},
archivePrefix = {arXiv},
arxivId = {2310.19793},
author = {Bietti, Alberto and Bruna, Joan and Pillaud-Vivien, Loucas},
eprint = {2310.19793},
title = {{On Learning Gaussian Multi-index Models with Gradient Flow}},
url = {http://arxiv.org/abs/2310.19793},
year = {2023}
}
@article{Ba2022,
abstract = {We study the first gradient descent step on the first-layer parameters W in a two-layer neural network: (equation presented), where W ∈ Rd×N,a ∈ RN are randomly initialized, and the training objective is the empirical MSE loss: (equation presented). In the proportional asymptotic limit where n,d,N → ∞ at the same rate, and an idealized student-teacher setting where the teacher f∗ is a single-index model, we compute the prediction risk of ridge regression on the conjugate kernel after one gradient step on W with learning rate $\eta$. We consider two scalings of the first step learning rate $\eta$. For small $\eta$, we establish a Gaussian equivalence property for the trained feature map, and prove that the learned kernel improves upon the initial random feature model, but cannot defeat the best linear model on the input. Whereas for sufficiently large $\eta$, we prove that for certain f∗, the same ridge estimator on trained features can go beyond this “linear regime” and outperform a wide range of (fixed) kernels. Our results demonstrate that even one gradient step can lead to a considerable advantage over random features, and highlight the role of learning rate scaling in the initial phase of training.},
archivePrefix = {arXiv},
arxivId = {2205.01445},
author = {Ba, Jimmy and Erdogdu, Murat A. and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
eprint = {2205.01445},
isbn = {9781713871088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation}},
volume = {35},
year = {2022}
}
@article{Bietti2022,
abstract = {Single-index models are a class of functions given by an unknown univariate “link” function applied to an unknown one-dimensional projection of the input. These models are particularly relevant in high dimension, when the data might present low-dimensional structure that learning algorithms should adapt to. While several statistical aspects of this model, such as the sample complexity of recovering the relevant (one-dimensional) subspace, are well-understood, they rely on tailored algorithms that exploit the specific structure of the target function. In this work, we introduce a natural class of shallow neural networks and study its ability to learn single-index models via gradient flow. More precisely, we consider shallow networks in which biases of the neurons are frozen at random initialization. We show that the corresponding optimization landscape is benign, which in turn leads to generalization guarantees that match the near-optimal sample complexity of dedicated semi-parametric methods.},
author = {Bietti, Alberto and Bruna, Joan and Sanford, Clayton and Song, Min Jae},
isbn = {9781713871088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Learning Single-Index Models with Shallow Neural Networks}},
volume = {35},
year = {2022}
}
@article{Dandi2024a,
abstract = {We investigate the training dynamics of two-layer neural networks when learning multi-index target functions. We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent. In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021). We characterize the (broad) class of functions efficiently learned in finite time. The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description of the dynamical process of the low-dimensional projections of the weights, and numerical experiments illustrating the theory.},
author = {Dandi, Yatin and Troiani, Emanuele and Arnaboldi, Luca and Pesce, Luca and Zdeborova, Lenka and Krzakala, Florent},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
pages = {9991--10016},
title = {{The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents}},
volume = {235},
year = {2024}
}
@article{Moniri2024,
abstract = {Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer can lead to feature learning; characterized by the appearance of a separated rank-one component-spike-in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By precisely analyzing the improvement in the training and test errors, we demonstrate that these non-linear features can enhance learning.},
archivePrefix = {arXiv},
arxivId = {2310.07891},
author = {Moniri, Behrad and Lee, Donghwan and Hassani, Hamed and Dobriban, Edgar},
eprint = {2310.07891},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
pages = {36106--36159},
title = {{A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks}},
volume = {235},
year = {2024}
}
@article{Ba2023a,
abstract = {We consider the problem of learning a single-index target function f∗ : Rd → R under the spiked covariance data: (Equation presented) where the link function $\sigma$∗ : R → R is a degree-p polynomial with information exponent k (defined as the lowest degree in the Hermite expansion of $\sigma$∗), and it depends on the projection of input x onto the spike (signal) direction µ ∈ Rd. In the proportional asymptotic limit where the number of training examples n and the dimensionality d jointly diverge: (Equation presented), we ask the following question: how large should the spike magnitude $\theta$ be, in order for (i) kernel methods, (ii) neural networks optimized by gradient descent, to learn f∗? We show that for kernel ridge regression, $\beta$ ≥ 1 − 1/p is both sufficient and necessary. Whereas for two-layer neural networks trained with gradient descent, $\beta$ > 1 − 1/k suffices. Our results demonstrate that both kernel methods and neural networks benefit from low-dimensional structures in the data. Further, since k ≤ p by definition, neural networks can adapt to such structures more effectively.},
author = {Ba, Jimmy and Erdogdu, Murat A. and Suzuki, Taiji and Wang, Zhichao and Wu, Denny},
isbn = {9781713899921},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Learning in the Presence of Low-dimensional Structure: A Spiked Random Matrix Perspective}},
volume = {36},
year = {2023}
}
@article{Dyer2020,
abstract = {Understanding the asymptotic behavior of wide networks is of considerable interest. In this work, we present a general method for analyzing this large width behavior. The method is an adaptation of Feynman diagrams, a standard tool for computing multivariate Gaussian integrals. We apply our method to study training dynamics, improving existing bounds and deriving new results on wide network evolution during stochastic gradient descent. Going beyond the strict large width limit, we present closed-form expressions for higher-order terms governing wide network training, and test these predictions empirically.},
archivePrefix = {arXiv},
arxivId = {1909.11304},
author = {Dyer, Ethan and Gur-Ari, Guy},
eprint = {1909.11304},
journal = {8th International Conference on Learning Representations, ICLR 2020},
title = {{Asymptotics of Wide Networks From Feynman Diagrams}},
year = {2020}
}
@article{Cui2024,
abstract = {In this manuscript, we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging the insight from (Ba et al., 2022), we model the trained network by a spiked Random Features (sRF) model. Further building on recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error of the sRF in the high-dimensional limit where the number of samples, the width, and the input dimension grow at a proportional rate. The resulting characterization for sRFs also captures closely the learning curves of the original network model. This enables us to understand how adapting to the data is crucial for the network to efficiently learn nonlinear functions in the direction of the gradient - where at initialization it can only express linear functions in this regime.},
author = {Cui, Hugo and Pesce, Luca and Dandi, Yatin and Krzakala, Florent and Lu, Yue M. and Zdeborov{\'{a}}, Lenka and Loureiro, Bruno},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
pages = {9662--9695},
title = {{Asymptotics of Feature Learning in Two-layer Networks after One Gradient-step}},
volume = {235},
year = {2024}
}
@article{Damian2022a,
abstract = {Significant theoretical work has established that in specific regimes, neural networks trained by gradient descent behave like kernel methods. However, in practice, it is known that neural networks strongly outperform their associated kernels. In this work, we explain this gap by demonstrating that there is a large class of functions which cannot be efficiently learned by kernel methods but can be easily learned with gradient descent on a two layer neural network outside the kernel regime by learning representations that are relevant to the target task. We also demonstrate that these representations allow for efficient transfer learning, which is impossible in the kernel regime. Specifically, we consider the problem of learning polynomials which depend on only a few relevant directions, i.e. of the form f?(x) = g(Ux) where U : Rd → Rr with d ≫ r. When the degree of f? is p, it is known that n dp samples are necessary to learn f? in the kernel regime. Our primary result is that gradient descent learns a representation of the data which depends only on the directions relevant to f?. This results in an improved sample complexity of n d2 and enables transfer learning with sample complexity independent of d.},
archivePrefix = {arXiv},
arxivId = {2206.15144},
author = {Damian, Alex and Lee, Jason D. and Soltanolkotabi, Mahdi},
eprint = {2206.15144},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
keywords = {gradient descent,kernel,neural network,representation learning,transfer learning},
pages = {5413--5452},
title = {{Neural Networks can Learn Representations with Gradient Descent}},
volume = {178},
year = {2022}
}
@article{Mousavi-Hosseini2023,
abstract = {Recent works have demonstrated that the sample complexity of gradient-based learning of single index models, i.e. functions that depend on a 1-dimensional projection of the input data, is governed by their information exponent. However, these results are only concerned with isotropic data, while in practice the input often contains additional structure which can implicitly guide the algorithm. In this work, we investigate the effect of a spiked covariance structure and reveal several interesting phenomena. First, we show that in the anisotropic setting, the commonly used spherical gradient dynamics may fail to recover the true direction, even when the spike is perfectly aligned with the target direction. Next, we show that appropriate weight normalization that is reminiscent of batch normalization can alleviate this issue. Further, by exploiting the alignment between the (spiked) input covariance and the target, we obtain improved sample complexity compared to the isotropic case. In particular, under the spiked model with a suitably large spike, the sample complexity of gradient-based training can be made independent of the information exponent while also outperforming lower bounds for rotationally invariant kernel methods.},
author = {Mousavi-Hosseini, Alireza and Wu, Denny and Suzuki, Taiji and Erdogdu, Murat A.},
isbn = {9781713899921},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Gradient-Based Feature Learning under Structured Data}},
volume = {36},
year = {2023}
}
@article{Hanin2020,
abstract = {We prove the precise scaling, at finite depth and width, for the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network. The standard deviation is exponential in the ratio of network depth to width. Thus, even in the limit of infinite overparameterization, the NTK is not deterministic if depth and width simultaneously tend to infinity. Moreover, we prove that for such deep and wide networks, the NTK has a non-trivial evolution during training by showing that the mean of its first SGD update is also exponential in the ratio of network depth to width. This is sharp contrast to the regime where depth is fixed and network width is very large. Our results suggest that, unlike relatively shallow and wide networks, deep and wide ReLU networks are capable of learning data-dependent features even in the so-called lazy training regime.},
archivePrefix = {arXiv},
arxivId = {1909.05989},
author = {Hanin, Boris and Nica, Mihai},
eprint = {1909.05989},
journal = {8th International Conference on Learning Representations, ICLR 2020},
title = {{Finite Depth and Width Corrections To the Neural Tangent Kernel}},
year = {2020}
}
@article{Naveh2020,
abstract = {A recent line of studies has focused on the infinite width limit of deep neural networks (DNNs) where, under a certain deterministic training protocol, the DNN outputs are related to a Gaussian Process (GP) known as the Neural Tangent Kernel (NTK). However, finite-width DNNs differ from GPs quantitatively and for CNNs the difference may be qualitative. Here we present a DNN training protocol involving noise whose outcome is mappable to a certain non-Gaussian stochastic process. An analytical framework is then introduced to analyze this resulting non-Gaussian process, whose deviation from a GP is controlled by the finite width. Our work extends upon previous relations between DNNs and GPs in several ways: (a) In the infinite width limit, it establishes a mapping between DNNs and a GP different from the NTK. (b) It allows computing analytically the general form of the finite width correction (FWC) for DNNs with arbitrary activation functions and depth and further provides insight on the magnitude and implications of these FWCs. (c) It appears capable of providing better performance than the corresponding GP in the case of CNNs. We are able to predict the outputs of empirical finite networks with high accuracy, improving upon the accuracy of GP predictions by over an order of magnitude. Overall, we provide a framework that offers both an analytical handle and a more faithful model of real-world settings than previous studies in this avenue of research.},
archivePrefix = {arXiv},
arxivId = {2004.01190},
author = {Naveh, Gadi and David, Oded Ben and Sompolinsky, Haim and Ringel, Zohar and Ben-David, Oded and Sompolinsky, Haim and Ringel, Zohar},
eprint = {2004.01190},
issn = {23318422},
journal = {arXiv},
pages = {1--24},
title = {{Predicting the outputs of finite networks trained with noisy gradients}},
url = {http://arxiv.org/abs/2004.01190},
year = {2020}
}
@article{Naveh2021,
abstract = {Deep neural networks (DNNs) in the infinite width/channel limit have received much attention recently, as they provide a clear analytical window to deep learning via mappings to Gaussian Processes (GPs). Despite its theoretical appeal, this viewpoint lacks a crucial ingredient of deep learning in finite DNNs, laying at the heart of their success -- feature learning. Here we consider DNNs trained with noisy gradient descent on a large training set and derive a self consistent Gaussian Process theory accounting for strong finite-DNN and feature learning effects. Applying this to a toy model of a two-layer linear convolutional neural network (CNN) shows good agreement with experiments. We further identify, both analytical and numerically, a sharp transition between a feature learning regime and a lazy learning regime in this model. Strong finite-DNN effects are also derived for a non-linear two-layer fully connected network. Our self consistent theory provides a rich and versatile analytical framework for studying feature learning and other non-lazy effects in finite DNNs.},
archivePrefix = {arXiv},
arxivId = {2106.04110},
author = {Naveh, Gadi and Ringel, Zohar},
eprint = {2106.04110},
month = {jun},
title = {{A self consistent theory of Gaussian Processes captures feature learning effects in finite CNNs}},
url = {http://arxiv.org/abs/2106.04110},
year = {2021}
}
@article{Refinetti2021,
abstract = {A recent series of theoretical works showed that the dynamics of neural networks with a certain initialisation are well-captured by kernel methods. Concurrent empirical work demonstrated that kernel methods can come close to the performance of neural networks on some image classification tasks.These results raise the question of whether neural networks only learn successfully if kernels also learn successfully, despite neural nets being more expressive. Here, we show theoretically that two-layer neural networks (2LNN) with only a few neurons can beat the performance of kernel learning on a simple Gaussian mixture classification task. We study the high-dimensional limit, i.e. when the number of samples is linearly proportional to the dimension, and show that while small 2LNN achieve near-optimal performance on this task, lazy training approaches such as random features and kernel methods do not.Our analysis is based on the derivation of a closed set of equations that track the learning dynamics of the 2LNN and thus allow to extract the asymptotic performance of the network as a function of signal-to-noise ratio and other hyperparameters. We finally illustrate how over-parametrising the neural network leads to faster convergence, but does not improve its final performance.},
archivePrefix = {arXiv},
arxivId = {2102.11742},
author = {Refinetti, Maria and Goldt, Sebastian and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
eprint = {2102.11742},
isbn = {9781713845065},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
pages = {8936--8947},
title = {{Classifying high-dimensional Gaussian mixtures: Where kernel methods fail and neural networks succeed}},
volume = {139},
year = {2021}
}
@article{Yehudai2019,
abstract = {Recently, a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient over-parameterization, gradient-based methods will implicitly leave some components of the network relatively unchanged, so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact, fixing these explicitly leads to the well-known approach of learning with random features (e.g. [27, 29]). In other words, these techniques imply that we can successfully learn with neural networks, whenever we can successfully learn with random features. In this paper, we formalize the link between existing results and random features, and argue that despite the impressive positive results, random feature approaches are also inherently limited in what they can explain. In particular, we prove that random features cannot be used to learn even a single ReLU neuron (over standard Gaussian inputs in Rd and poly(d) weights), unless the network size (or magnitude of its weights) is exponentially large in d. Since a single neuron is known to be learnable with gradient-based methods, we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks. For completeness we also provide a simple self-contained proof, using a random features technique, that one-hidden-layer neural networks can learn low-degree polynomials.},
archivePrefix = {arXiv},
arxivId = {1904.00687},
author = {Yehudai, Gilad and Shamir, Ohad},
eprint = {1904.00687},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{On the power and limitations of random features for understanding neural networks}},
volume = {32},
year = {2019}
}
@article{Arous2021,
abstract = {Stochastic gradient descent (SGD) is a popular algorithm for optimization problems arising in high-dimensional inference tasks. Here one produces an estimator of an unknown parameter from independent samples of data by iteratively optimizing a loss function. This loss function is random and often non-convex. We study the performance of the simplest version of SGD, namely online SGD, from a random start in the setting where the parameter space is high-dimensional. We develop nearly sharp thresholds for the number of samples needed for consistent estimation as one varies the dimension. Our thresholds depend only on an intrinsic property of the population loss which we call the information exponent. In particular, our results do not assume uniform control on the loss itself, such as convexity or uniform derivative bounds. The thresholds we obtain are polynomial in the dimension and the precise exponent depends explicitly on the information exponent. As a consequence of our results, we find that except for the simplest tasks, almost all of the data is used simply in the initial search phase to obtain non-trivial correlation with the ground truth. Upon attaining nontrivial correlation, the descent is rapid and exhibits law of large numbers type behavior. We illustrate our approach by applying it to a wide set of inference tasks such as phase retrieval, and parameter estimation for generalized linear models, online PCA, and spiked tensor models, as well as to supervised learning for single-layer networks with general activation functions.},
archivePrefix = {arXiv},
arxivId = {2003.10409},
author = {Arous, Gerard Ben and Gheissari, Reza and Jagannath, Aukosh},
eprint = {2003.10409},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Generalized linear models,Non-convex optimization,Parameter estimation,Stochastic gradient descent,Supervised learning,Tensor PCA},
title = {{Online stochastic gradient descent on non-convex losses from high-dimensional inference}},
volume = {22},
year = {2021}
}
@article{Ba2023,
abstract = {We consider the problem of learning a single-index target function f∗ : Rd → R under the spiked covariance data: (Equation presented) where the link function $\sigma$∗ : R → R is a degree-p polynomial with information exponent k (defined as the lowest degree in the Hermite expansion of $\sigma$∗), and it depends on the projection of input x onto the spike (signal) direction µ ∈ Rd. In the proportional asymptotic limit where the number of training examples n and the dimensionality d jointly diverge: (Equation presented), we ask the following question: how large should the spike magnitude $\theta$ be, in order for (i) kernel methods, (ii) neural networks optimized by gradient descent, to learn f∗? We show that for kernel ridge regression, $\beta$ ≥ 1 − 1/p is both sufficient and necessary. Whereas for two-layer neural networks trained with gradient descent, $\beta$ > 1 − 1/k suffices. Our results demonstrate that both kernel methods and neural networks benefit from low-dimensional structures in the data. Further, since k ≤ p by definition, neural networks can adapt to such structures more effectively.},
author = {Ba, Jimmy and Erdogdu, Murat A. and Suzuki, Taiji and Wang, Zhichao and Wu, Denny},
isbn = {9781713899921},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Learning in the Presence of Low-dimensional Structure: A Spiked Random Matrix Perspective}},
volume = {36},
year = {2023}
}
@article{Dandi2024,
abstract = {For high-dimensional Gaussian data, we investigate theoretically how the features of a two-layer neural network adapt to the structure of the target function through a few large batch gradient descent steps, leading to an improvement in the approximation capacity from initialization. First, we compare the influence of batch size to that of multiple steps. For a single step, a batch of size $n = \mathcal{O}(d)$ is both necessary and sufficient to align with the target function, although only a single direction can be learned. In contrast, $n = \mathcal{O}(d^2)$ is essential for neurons to specialize in multiple relevant directions of the target with a single gradient step. Even in this case, we show there might exist ``hard'' directions requiring $n = \mathcal{O}(d^\ell)$ samples to be learned, where $\ell$ is known as the leap index of the target. Second, we show that the picture drastically improves over multiple gradient steps: a batch size of $n = \mathcal{O}(d)$ is indeed sufficient to learn multiple target directions satisfying a staircase property, where more and more directions can be learned over time. Finally, we discuss how these directions allow for a drastic improvement in the approximation capacity and generalization error over the initialization, illustrating a separation of scale between the random features/lazy regime and the feature learning regime. Our technical analysis leverages a combination of techniques related to concentration, projection-based conditioning, and Gaussian equivalence, which we believe are of independent interest. By pinning down the conditions necessary for specialization and learning, our results highlight the intertwined role of the structure of the task to learn, the details of the algorithm, and the architecture, shedding new light on how neural networks adapt to the feature and learn complex task from data over time.},
archivePrefix = {arXiv},
arxivId = {2305.18270},
author = {Dandi, Yatin and Krzakala, Florent and Loureiro, Bruno and Pesce, Luca and Stephan, Ludovic},
eprint = {2305.18270},
journal = {Journal of Machine Learning Research},
month = {jun},
number = {1},
pages = {17099 -- 17163},
title = {{How Two-Layer Neural Networks Learn, One (Giant) Step at a Time}},
volume = {25},
year = {2024}
}
@article{Abbe2023,
abstract = {We investigate the time complexity of SGD learning on fully-connected neural networks with isotropic data. We put forward a complexity measure, the leap, which measures how “hierarchical” target functions are. For d-dimensional uniform Boolean or isotropic Gaussian data, our main conjecture states that the time complexity to learn a function f with low-dimensional support is $\Theta$̃(dmax(Leap(f),2)) . We prove a version of this conjecture for a class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how SGD is run. We show that the training sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from Abbe et al. (2022b) by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here. Finally, we note that this gives an SGD complexity for the full training trajectory that matches that of Correlational Statistical Query (CSQ) lower-bounds.},
archivePrefix = {arXiv},
arxivId = {2302.11055},
author = {Abbe, Emmanuel and Boix-Adser{\`{a}}, Enric and Misiakiewicz, Theodor},
eprint = {2302.11055},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
pages = {2552--2623},
title = {{SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics}},
volume = {195},
year = {2023}
}
@article{Damian2022,
abstract = {Significant theoretical work has established that in specific regimes, neural networks trained by gradient descent behave like kernel methods. However, in practice, it is known that neural networks strongly outperform their associated kernels. In this work, we explain this gap by demonstrating that there is a large class of functions which cannot be efficiently learned by kernel methods but can be easily learned with gradient descent on a two layer neural network outside the kernel regime by learning representations that are relevant to the target task. We also demonstrate that these representations allow for efficient transfer learning, which is impossible in the kernel regime. Specifically, we consider the problem of learning polynomials which depend on only a few relevant directions, i.e. of the form f?(x) = g(Ux) where U : Rd → Rr with d ≫ r. When the degree of f? is p, it is known that n dp samples are necessary to learn f? in the kernel regime. Our primary result is that gradient descent learns a representation of the data which depends only on the directions relevant to f?. This results in an improved sample complexity of n d2 and enables transfer learning with sample complexity independent of d.},
archivePrefix = {arXiv},
arxivId = {2206.15144},
author = {Damian, Alex and Lee, Jason D. and Soltanolkotabi, Mahdi},
eprint = {2206.15144},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
keywords = {gradient descent,kernel,neural network,representation learning,transfer learning},
pages = {5413--5452},
title = {{Neural Networks can Learn Representations with Gradient Descent}},
volume = {178},
year = {2022}
}
@article{Yang2020a,
abstract = {As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases. More generally, we classify a natural space of neural network parametrizations that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/TP4.},
archivePrefix = {arXiv},
arxivId = {2011.14522},
author = {Yang, Greg and Hu, Edward J.},
eprint = {2011.14522},
journal = {arXiv:2011.14522},
pages = {65},
title = {{Feature Learning in Infinite-Width Neural Networks}},
url = {https://arxiv.org/abs/2011.14522v3},
year = {2020}
}
@article{Berthier2024,
abstract = {Gradient-based learning in multi-layer neural networks displays a number of striking features. In particular, the decrease rate of empirical risk is non-monotone even after averaging over large batches. Long plateaus in which one observes barely any progress alternate with intervals of rapid decrease. These successive phases of learning often take place on very different time scales. Finally, models learnt in an early phase are typically ‘simpler' or ‘easier to learn' although in a way that is difficult to formalize. Although theoretical explanations of these phenomena have been put forward, each of them captures at best certain specific regimes. In this paper, we study the gradient flow dynamics of a wide two-layer neural network in high-dimension, when data are distributed according to a single-index model (i.e., the target function depends on a one-dimensional projection of the covariates). Based on a mixture of new rigorous results, non-rigorous mathematical derivations, and numerical simulations, we propose a scenario for the learning dynamics in this setting. In particular, the proposed evolution exhibits separation of timescales and intermittency. These behaviors arise naturally because the population gradient flow can be recast as a singularly perturbed dynamical system.},
archivePrefix = {arXiv},
arxivId = {2303.00055},
author = {Berthier, Rapha{\"{e}}l and Montanari, Andrea and Zhou, Kangjie},
doi = {10.1007/s10208-024-09664-9},
eprint = {2303.00055},
issn = {16153383},
journal = {Foundations of Computational Mathematics},
keywords = {34E15,37N40,68T07,Deep learning,Dynamical system,Gradient flow,Incremental learning,Neural network,Non-convex optimization},
title = {{Learning Time-Scales in Two-Layers Neural Networks}},
year = {2024}
}
@article{Blanco-Justicia2023,
abstract = {We review the use of differential privacy (DP) for privacy protection in machine learning (ML). We show that, driven by the aim of preserving the accuracy of the learned models, DP-based ML implementations are so loose that they do not offer the ex ante privacy guarantees of DP. Instead, what they deliver is basically noise addition similar to the traditional (and often criticized) statistical disclosure control approach. Due to the lack of formal privacy guarantees, the actual level of privacy offered must be experimentally assessed ex post, which is done very seldom. In this respect, we present empirical results showing that standard anti-overfitting techniques in ML can achieve a better utility/privacy/efficiency tradeoff than DP.},
archivePrefix = {arXiv},
arxivId = {2206.04621},
author = {Blanco-Justicia, Alberto and S{\'{a}}nchez, David and Domingo-Ferrer, Josep and Muralidhar, Krishnamurty},
doi = {10.1145/3547139},
eprint = {2206.04621},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Differential privacy,data utility,federated learning,machine learning},
number = {8},
title = {{A Critical Review on the Use (and Misuse) of Differential Privacy in Machine Learning}},
volume = {55},
year = {2023}
}
@article{Dwork2006,
abstract = {We continue a line of research initiated in [10, 11] on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function / mapping databases to reals, the so-called true answer is the result of applying / to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user. Previous work focused on the case of noisy sums, in which f = ∑ i g(x i), where x i denotes the ith row of the database and g maps database rows to [0, 1]. We extend the study to general functions f, proving that privacy can be preserved by calibrating the standard deviation of the noise according to the sensitivity of the function f. Roughly speaking, this is the amount that any single argument to f can change its output. The new analysis shows that for several particular applications substantially less noise is needed than was previously understood to be the case. The first step is a very clean characterization of privacy in terms of indistinguishability of transcripts. Additionally, we obtain separation results showing the increased value of interactive sanitization mechanisms over non-interactive. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
doi = {10.1007/11681878_14},
isbn = {3540327312},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {265--284},
title = {{Calibrating noise to sensitivity in private data analysis}},
volume = {3876 LNCS},
year = {2006}
}
@article{Martinetz2025,
author = {Martinetz, Julius and Martinetz, Thomas},
doi = {10.1109/TNNLS.2025.3529297},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pages = {1--11},
title = {{Do Highly Over-Parameterized Neural Networks Generalize Since Bad Solutions Are Rare?}},
url = {https://ieeexplore.ieee.org/document/10851347/},
year = {2025}
}
@article{Li2020,
abstract = {Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.},
archivePrefix = {arXiv},
arxivId = {2002.11794},
author = {Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joseph E.},
eprint = {2002.11794},
isbn = {9781713821120},
journal = {37th International Conference on Machine Learning, ICML 2020},
pages = {5914--5924},
title = {{Train Large, then compress: Rethinking model size for efficient training and inference of transformers}},
volume = {PartF16814},
year = {2020}
}
@article{Abadi2016,
abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
archivePrefix = {arXiv},
arxivId = {1607.00133},
author = {Abadi, Mart{\'{i}}n and McMahan, H. Brendan and Chu, Andy and Mironov, Ilya and Zhang, Li and Goodfellow, Ian and Talwar, Kunal},
doi = {10.1145/2976749.2978318},
eprint = {1607.00133},
isbn = {9781450341394},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
pages = {308--318},
title = {{Deep learning with differential privacy}},
volume = {24-28-Octo},
year = {2016}
}
@article{Papernot2021,
abstract = {Because learning sometimes involves sensitive data, machine learning algorithms have been extended to offer differential privacy for training data. In practice, this has been mostly an afterthought, with privacy-preserving models obtained by re-running training with a different optimizer, but using the model architectures that already performed well in a non-privacy-preserving setting. This approach leads to less than ideal privacy/utility tradeoffs, as we show here. To improve these tradeoffs, prior work introduces variants of differential privacy that weaken the privacy guarantee proved to increase model utility. We show this is not necessary and instead propose that utility be improved by choosing activation functions designed explicitly for privacy-preserving training. A crucial operation in differentially private SGD is gradient clipping, which along with modifying the optimization path (at times resulting in not-optimizing a single objective function), may also introduce both significant bias and variance to the learning process. We empirically identify exploding gradients arising from ReLU may be one of the main sources of this. We demonstrate analytically and experimentally how a general family of bounded activation functions, the tempered sigmoids, consistently outperform the currently established choice: unbounded activation functions like ReLU. Using this paradigm, we achieve new state-of-the-art accuracy on MNIST, FashionMNIST, and CIFAR10 without any modification of the learning procedure fundamentals or differential privacy analysis. While the changes we make are simple in retrospect, the simplicity of our approach facilitates its implementation and adoption to meaningfully improve state-of-the-art machine learning while still providing strong guarantees in the original framework of differential privacy.},
archivePrefix = {arXiv},
arxivId = {2007.14191},
author = {Papernot, Nicolas and Thakurta, Abhradeep and Song, Shuang and Chien, Steve and Erlingsson, {\'{U}}lfar},
doi = {10.1609/aaai.v35i10.17123},
eprint = {2007.14191},
isbn = {9781713835974},
issn = {2159-5399},
journal = {35th AAAI Conference on Artificial Intelligence, AAAI 2021},
pages = {9312--9321},
title = {{Tempered Sigmoid Activations for Deep Learning with Differential Privacy}},
volume = {10B},
year = {2021}
}
@article{Tramer2021,
abstract = {We demonstrate that differentially private machine learning has not yet reached its “AlexNet moment” on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets. To exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain. Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.},
archivePrefix = {arXiv},
arxivId = {2011.11660},
author = {Tram{\`{e}}r, Florian and Boneh, Dan},
eprint = {2011.11660},
journal = {ICLR 2021 - 9th International Conference on Learning Representations},
title = {{Differentially Private Learning Needs Better Features (or Much More Data)}},
year = {2021}
}
@article{Bosch2023,
abstract = {We provide exact asymptotic expressions for the performance of regression by an L−layer deep random feature (RF) model, where the input is mapped through multiple random embedding and non-linear activation functions. For this purpose, we establish two key steps: First, we prove a novel universality result for RF models and deterministic data, by which we demonstrate that a deep random feature model is equivalent to a deep linear Gaussian model that matches it in the first and second moments, at each layer. Second, we make use of the convex Gaussian Min-Max theorem multiple times to obtain the exact behavior of deep RF models. We further characterize the variation of the eigendistribution in different layers of the equivalent Gaussian model, demonstrating that depth has a tangible effect on model performance despite the fact that only the last layer of the model is being trained.},
archivePrefix = {arXiv},
arxivId = {2302.06210},
author = {Bosch, David and Panahi, Ashkan and Hassibi, Babak},
eprint = {2302.06210},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
keywords = {Asymptotic Analysis,Convex Gaussian Min Max Theorem,Learning Curves,Random Features Model,Universality},
pages = {4132--4179},
title = {{Precise Asymptotic Analysis of Deep Random Feature Models}},
volume = {195},
year = {2023}
}
@article{Hu2023,
abstract = {We prove a universality theorem for learning with random features. Our result shows that, in terms of training and generalization errors, a random feature model with a nonlinear activation function is asymptotically equivalent to a surrogate linear Gaussian model with a matching covariance matrix. This settles a so-called Gaussian equivalence conjecture based on which several recent papers develop their results. Our method for proving the universality theorem builds on the classical Lindeberg approach. Major ingredients of the proof include a leave-one-out analysis for the optimization problem associated with the training process and a central limit theorem, obtained via Stein's method, for weakly correlated random variables.},
archivePrefix = {arXiv},
arxivId = {2009.07669},
author = {Hu, Hong and Lu, Yue M.},
doi = {10.1109/TIT.2022.3217698},
eprint = {2009.07669},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
keywords = {Gaussian equivalence,Random feature model,exact asymptotics,overparameterized neural network,universality},
number = {3},
pages = {1932--1964},
title = {{Universality Laws for High-Dimensional Learning With Random Features}},
volume = {69},
year = {2023}
}
@article{Dwork2013,
abstract = {The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition. After motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some astonishingly powerful computational results, there are still fundamental limitations - not just on what can be achieved with differential privacy but on what can be achieved with any method that protects against a complete breakdown in privacy. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power. Certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed. We then turn from fundamentals to applications other than queryrelease, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams is discussed. Finally, we note that this work is meant as a thorough introduction to the problems and techniques of differential privacy, but is not intended to be an exhaustive survey- there is by now a vast amount of work in differential privacy, and we can cover only a small portion of it. {\textcopyright} 2014 C. Dwork and A. Roth.},
author = {Dwork, Cynthia and Roth, Aaron},
doi = {10.1561/0400000042},
issn = {15513068},
journal = {Foundations and Trends in Theoretical Computer Science},
number = {3-4},
pages = {211--487},
title = {{The algorithmic foundations of differential privacy}},
volume = {9},
year = {2013}
}
@article{Geman1992,
abstract = {Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.},
author = {Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'{e}}},
doi = {10.1162/neco.1992.4.1.1},
issn = {0899-7667},
journal = {Neural Computation},
month = {jan},
number = {1},
pages = {1--58},
title = {{Neural Networks and the Bias/Variance Dilemma}},
url = {https://direct.mit.edu/neco/article/4/1/1-58/5624},
volume = {4},
year = {1992}
}
@book{Sapatinas2004,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
author = {Sapatinas, Theofanis},
booktitle = {Journal of the Royal Statistical Society Series A: Statistics in Society},
doi = {10.1111/j.1467-985x.2004.298_11.x},
issn = {0964-1998},
number = {1},
pages = {192--192},
title = {{The Elements of Statistical Learning}},
volume = {167},
year = {2004}
}
@inproceedings{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
address = {San Diego, CA, USA},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
doi = {arXiv:1409.1556v6},
eprint = {1409.1556},
month = {apr},
title = {{Very deep convolutional networks for large-scale image recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Yang2020,
abstract = {The classical bias-variance trade-off predicts that bias decreases and variance increases with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.},
archivePrefix = {arXiv},
arxivId = {2002.11328},
author = {Yang, Zitong and Yu, Yaodong and You, Chong and Steinhardt, Jacob and Ma, Yi},
eprint = {2002.11328},
isbn = {9781713821120},
journal = {37th International Conference on Machine Learning, ICML 2020},
pages = {10698--10708},
title = {{Rethinking bias-variance trade-off for generalization of neural networks}},
volume = {PartF16814},
year = {2020}
}
@article{Krizhevsky2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
issn = {15577317},
journal = {Communications of the ACM},
month = {may},
number = {6},
pages = {84--90},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {https://dl.acm.org/doi/10.1145/3065386},
volume = {60},
year = {2017}
}
@inproceedings{Liu2015,
abstract = {Since Krizhevsky won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 competition with the brilliant deep convolutional neural networks(D-CNNs), researchers have designed lots of D-CNNs. However, almost all the existing very deep convolutional neural networks are trained on the giant ImageNet datasets. Small datasets like CIFAR-10 has rarely taken advantage of the power of depth since deep models are easy to overfit. In this paper, we proposed a modified VGG-16 network and used this model to fit CIFAR-10. By adding stronger regularizer and using Batch Normalization, we achieved 8.45% error rate on CIFAR-10 without severe overfitting. Our results show that the very deep CNN can be used to fit small datasets with simple and proper modifications and don't need to re-design specific small networks. We believe that if a model is strong enough to fit a large dataset, it can also fit a small one.},
author = {Liu, Shuying and Deng, Weihong},
booktitle = {Proceedings - 3rd IAPR Asian Conference on Pattern Recognition, ACPR 2015},
doi = {10.1109/ACPR.2015.7486599},
isbn = {9781479961009},
month = {nov},
pages = {730--734},
publisher = {IEEE},
title = {{Very deep convolutional neural network based image classification using small training sample size}},
url = {http://ieeexplore.ieee.org/document/7486599/},
year = {2016}
}
@article{Hardt2016,
abstract = {We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit.},
archivePrefix = {arXiv},
arxivId = {1509.01240},
author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
eprint = {1509.01240},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {1868--1877},
title = {{Train faster, generalize better: Stability of stochastic gradient descent}},
volume = {3},
year = {2016}
}
@article{Neyshabur2015,
abstract = {We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multi-layer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.},
archivePrefix = {arXiv},
arxivId = {1412.6614},
author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
eprint = {1412.6614},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Workshop Track Proceedings},
title = {{In search of the real inductive bias: On the role of implicit regularization in deep learning}},
year = {2015}
}
@article{Cleophas2013,
abstract = {Machine learning is a novel discipline concerned with the analysis of large and multiple variables data. It involves computationally intensive methods, like factor analysis, cluster analysis, and discriminant analysis. It is currently mainly the domain of computer scientists, and is already commonly used in social sciences, marketing research, operational research and applied sciences. It is virtually unused in clinical research. This is probably due to the traditional belief of clinicians in clinical trials where multiple variables are equally balanced by the randomization process and are not further taken into account. In contrast, modern computer data files often involve hundreds of variables like genes and other laboratory values, and computationally intensive methods are required. This book was written as a hand-hold presentation accessible to clinicians, and as a must-read publication for those new to the methods.},
author = {Cleophas, Ton J. and Zwinderman, Aeilko H.},
doi = {10.1007/978-94-007-5824-7},
isbn = {9789400758247},
journal = {Machine Learning in Medicine},
pages = {1--265},
title = {{Machine learning in medicine}},
volume = {9789400758},
year = {2013}
}
@article{Dada2019,
abstract = {The upsurge in the volume of unwanted emails called spam has created an intense need for the development of more dependable and robust antispam filters. Machine learning methods of recent are being used to successfully detect and filter spam emails. We present a systematic review of some of the popular machine learning based email spam filtering approaches. Our review covers survey of the important concepts, attempts, efficiency, and the research trend in spam filtering. The preliminary discussion in the study background examines the applications of machine learning techniques to the email spam filtering process of the leading internet service providers (ISPs) like Gmail, Yahoo and Outlook emails spam filters. Discussion on general email spam filtering process, and the various efforts by different researchers in combating spam through the use machine learning techniques was done. Our review compares the strengths and drawbacks of existing machine learning approaches and the open research problems in spam filtering. We recommended deep leaning and deep adversarial learning as the future techniques that can effectively handle the menace of spam emails.},
author = {Dada, Emmanuel Gbenga and Bassi, Joseph Stephen and Chiroma, Haruna and Abdulhamid, Shafi'i Muhammad and Adetunmbi, Adebayo Olusola and Ajibuwa, Opeyemi Emmanuel},
doi = {10.1016/j.heliyon.2019.e01802},
issn = {24058440},
journal = {Heliyon},
keywords = {Analysis of algorithms,Computer privacy,Computer science,Computer security,Deep learning,Machine learning,Na{\"{i}}ve Bayes,Neural networks,Spam filtering,Support vector machines},
number = {6},
title = {{Machine learning for email spam filtering: review, approaches and open research problems}},
volume = {5},
year = {2019}
}
@article{Shehab2022,
abstract = {Applications of machine learning (ML) methods have been used extensively to solve various complex challenges in recent years in various application areas, such as medical, financial, environmental, marketing, security, and industrial applications. ML methods are characterized by their ability to examine many data and discover exciting relationships, provide interpretation, and identify patterns. ML can help enhance the reliability, performance, predictability, and accuracy of diagnostic systems for many diseases. This survey provides a comprehensive review of the use of ML in the medical field highlighting standard technologies and how they affect medical diagnosis. Five major medical applications are deeply discussed, focusing on adapting the ML models to solve the problems in cancer, medical chemistry, brain, medical imaging, and wearable sensors. Finally, this survey provides valuable references and guidance for researchers, practitioners, and decision-makers framing future research and development directions.},
author = {Shehab, Mohammad and Abualigah, Laith and Shambour, Qusai and Abu-Hashem, Muhannad A. and Shambour, Mohd Khaled Yousef and Alsalibi, Ahmed Izzat and Gandomi, Amir H.},
doi = {10.1016/j.compbiomed.2022.105458},
issn = {18790534},
journal = {Computers in Biology and Medicine},
keywords = {Diagnosis,Healthcare,Machine learning,Medical applications,Medical field},
pmid = {35364311},
title = {{Machine learning in medical applications: A review of state-of-the-art methods}},
volume = {145},
year = {2022}
}
@article{Weiner2024,
abstract = {This study presents a comprehensive overview of the approaches employed in recommendation systems over the last decade. The review primarily draws from two categories of filtering techniques: content-based filtering and collaborative filtering methods. We have reviewed and tabulated approximately forty articles that have been published. Major findings include: (1) collaborative filtering is more often used than content-based filtering, 70% to 23%, the rest is hybrid methods of these two; (2) more than half of the machine learning approaches adopted are supervised learning; however, (3) algorithm-wise, K-means the unsupervised learning algorithm emerged as the most frequently adopted approach in recommendation systems. Also notably, cosine similarity stands out as the prevalent measurement technique.},
author = {Weiner, Felix and Teh, Phoey Lee and Cheng, Chi Bin},
doi = {10.1007/978-3-031-62281-6_5},
isbn = {9783031622809},
issn = {23673389},
journal = {Lecture Notes in Networks and Systems},
keywords = {Collaborative filtering,Content-based filtering,K-Mean clustering techniques,Recommendation systems},
pages = {66--75},
title = {{Systematic Review of Machine Learning in Recommendation Systems Over the Last Decade}},
volume = {1016 LNNS},
year = {2024}
}
@article{HernandezAros2024,
abstract = {Financial fraud negatively impacts organizational administrative processes, particularly affecting owners and/or investors seeking to maximize their profits. Addressing this issue, this study presents a literature review on financial fraud detection through machine learning techniques. The PRISMA and Kitchenham methods were applied, and 104 articles published between 2012 and 2023 were examined. These articles were selected based on predefined inclusion and exclusion criteria and were obtained from databases such as Scopus, IEEE Xplore, Taylor & Francis, SAGE, and ScienceDirect. These selected articles, along with the contributions of authors, sources, countries, trends, and datasets used in the experiments, were used to detect financial fraud and its existing types. Machine learning models and metrics were used to assess performance. The analysis indicated a trend toward using real datasets. Notably, credit card fraud detection models are the most widely used for detecting credit card loan fraud. The information obtained by different authors was acquired from the stock exchanges of China, Canada, the United States, Taiwan, and Tehran, among other countries. Furthermore, the usage of synthetic data has been low (less than 7% of the employed datasets). Among the leading contributors to the studies, China, India, Saudi Arabia, and Canada remain prominent, whereas Latin American countries have few related publications.},
author = {{Hernandez Aros}, Ludivia and {Bustamante Molano}, Luisa Ximena and Gutierrez-Portela, Fernando and {Moreno Hernandez}, John Johver and {Rodr{\'{i}}guez Barrero}, Mario Samuel},
doi = {10.1057/s41599-024-03606-0},
issn = {26629992},
journal = {Humanities and Social Sciences Communications},
number = {1},
title = {{Financial fraud detection through the application of machine learning techniques: a literature review}},
volume = {11},
year = {2024}
}
@article{Carlini2021,
abstract = {It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.},
archivePrefix = {arXiv},
arxivId = {2012.07805},
author = {Carlini, Nicholas and Tram{\`{e}}r, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, {\'{U}}lfar and Oprea, Alina and Raffel, Colin},
eprint = {2012.07805},
isbn = {9781939133243},
journal = {Proceedings of the 30th USENIX Security Symposium},
pages = {2633--2650},
title = {{Extracting training data from large language models}},
year = {2021}
}
@article{Fredrikson2015,
abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al. [13], adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.},
author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
doi = {10.1145/2810103.2813677},
isbn = {9781450338325},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
pages = {1322--1333},
title = {{Model inversion attacks that exploit confidence information and basic countermeasures}},
volume = {2015-Octob},
year = {2015}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {448--456},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
volume = {1},
year = {2015}
}
@article{Zhang2017,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
author = {Zhang, Chiyuan and Recht, Benjamin and Bengio, Samy and Hardt, Moritz and Vinyals, Oriol},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
title = {{Understanding deep learning requires rethinking generalization}},
year = {2017}
}
@article{Hinton2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. {\textcopyright} 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Model combination,Neural networks,Regularization},
number = {1},
pages = {1929--1958},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@article{Cai2024,
abstract = {Representing signals using coordinate networks dominates the area of inverse problems recently, and is widely applied in various scientific computing tasks. Still, there exists an issue of spectral bias in coordinate networks, limiting the capacity to learn high-frequency components. This problem is caused by the pathological distribution of the neural tangent kernel's (NTK's) eigenvalues of coordinate networks. We find that, this pathological distribution could be improved using the classical batch normalization (BN), which is a common deep learning technique but rarely used in coordinate networks. BN greatly reduces the maximum and variance of NTK's eigenvalues while slightly modifies the mean value, considering the max eigenvalue is much larger than the most, this variance change results in a shift of eigenvalues' distribution from a lower one to a higher one, therefore the spectral bias could be alleviated (see Fig. 1). This observation is substantiated by the significant improvements of applying BN-based coordinate networks to various tasks, including the image compression, computed tomography reconstruction, shape representation, magnetic resonance imaging and novel view synthesis.},
author = {Cai, Zhicheng and Zhu, Hao and Shen, Qiu and Wang, Xinran and Cao, Xun},
doi = {10.1109/CVPR52733.2024.02377},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Coordinate Networks,Implicit Neural Representation,Normalization},
pages = {25160--25171},
title = {{Batch Normalization Alleviates the Spectral Bias in Coordinate Networks}},
year = {2024}
}
@article{Allen-Zhu2019,
abstract = {The fundamental learning theory behind neural networks remains largely open. What classes of functions can neural networks actually learn? Why doesn't the trained network overfit when it is overparameterized? In this work, we prove that overparameterized neural networks can learn some notable concept classes, including two and three-layer networks with fewer parameters and smooth activations. Moreover, the learning can be simply done by SGD (stochastic gradient descent) or its variants in polynomial time using polynomially many samples. The sample complexity can also be almost independent of the number of parameters in the network. On the technique side, our analysis goes beyond the so-called NTK (neural tangent kernel) linearization of neural networks in prior works. We establish a new notion of quadratic approximation of the neural network, and connect it to the SGD theory of escaping saddle points.},
archivePrefix = {arXiv},
arxivId = {1811.04918},
author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
eprint = {1811.04918},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Learning and generalization in overparameterized neural networks, going beyond two layers}},
volume = {32},
year = {2019}
}
@article{Belkin2019,
abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
doi = {10.1073/pnas.1903070116},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Bias–variance trade-off,Machine learning,Neural networks},
number = {32},
pages = {15849--15854},
pmid = {31341078},
title = {{Reconciling modern machine-learning practice and the classical bias–variance trade-off}},
volume = {116},
year = {2019}
}
@article{Kurakin2022,
abstract = {Differential privacy (DP) is the de facto standard for training machine learning (ML) models, including neural networks, while ensuring the privacy of individual examples in the training set. Despite a rich literature on how to train ML models with differential privacy, it remains extremely challenging to train real-life, large neural networks with both reasonable accuracy and privacy. We set out to investigate how to do this, using ImageNet image classification as a poster example of an ML task that is very challenging to resolve accurately with DP right now. This paper shares initial lessons from our effort, in the hope that it will inspire and inform other researchers to explore DP training at scale. We show approaches that help make DP training faster, as well as model types and settings of the training process that tend to work better in the DP setting. Combined, the methods we discuss let us train a Resnet-18 with DP to $47.9\%$ accuracy and privacy parameters $\epsilon = 10, \delta = 10^{-6}$. This is a significant improvement over "naive" DP training of ImageNet models, but a far cry from the $75\%$ accuracy that can be obtained by the same network without privacy. The model we use was pretrained on the Places365 data set as a starting point. We share our code at https://github.com/google-research/dp-imagenet, calling for others to build upon this new baseline to further improve DP at scale.},
archivePrefix = {arXiv},
arxivId = {2201.12328},
author = {Kurakin, Alexey and Song, Shuang and Chien, Steve and Geambasu, Roxana and Terzis, Andreas and Thakurta, Abhradeep},
eprint = {2201.12328},
title = {{Toward Training at ImageNet Scale with Differential Privacy}},
url = {http://arxiv.org/abs/2201.12328},
year = {2022}
}
@article{De2022,
abstract = {Differential Privacy (DP) provides a formal privacy guarantee preventing adversaries with access to a machine learning model from extracting information about individual training points. Differentially Private Stochastic Gradient Descent (DP-SGD), the most popular DP training method for deep learning, realizes this protection by injecting noise during training. However previous works have found that DP-SGD often leads to a significant degradation in performance on standard image classification benchmarks. Furthermore, some authors have postulated that DP-SGD inherently performs poorly on large models, since the norm of the noise required to preserve privacy is proportional to the model dimension. In contrast, we demonstrate that DP-SGD on over-parameterized models can perform significantly better than previously thought. Combining careful hyper-parameter tuning with simple techniques to ensure signal propagation and improve the convergence rate, we obtain a new SOTA without extra data on CIFAR-10 of 81.4% under (8, 10^{-5})-DP using a 40-layer Wide-ResNet, improving over the previous SOTA of 71.7%. When fine-tuning a pre-trained NFNet-F3, we achieve a remarkable 83.8% top-1 accuracy on ImageNet under (0.5, 8*10^{-7})-DP. Additionally, we also achieve 86.7% top-1 accuracy under (8, 8 \cdot 10^{-7})-DP, which is just 4.3% below the current non-private SOTA for this task. We believe our results are a significant step towards closing the accuracy gap between private and non-private image classification.},
archivePrefix = {arXiv},
arxivId = {2204.13650},
author = {De, Soham and Berrada, Leonard and Hayes, Jamie and Smith, Samuel L. and Balle, Borja},
eprint = {2204.13650},
title = {{Unlocking High-Accuracy Differentially Private Image Classification through Scale}},
url = {http://arxiv.org/abs/2204.13650},
year = {2022}
}
@article{Chaudhuri2009,
abstract = {This paper addresses the important tradeoff between privacy and learnability, when designing algorithms for learning from private databases. We focus on privacy-preserving logistic regression. First we apply an idea of Dwork et al. [6] to design a privacy-preserving logistic regression algorithm. This involves bounding the sensitivity of regularized logistic regression, and perturbing the learned classifier with noise proportional to the sensitivity. We then provide a privacy-preserving regularized logistic regression algorithm based on a new privacy-preserving technique: solving a perturbed optimization problem. We prove that our algorithm preserves privacy in the model due to [6]. We provide learning guarantees for both algorithms, which are tighter for our new algorithm, in cases in which one would typically apply logistic regression. Experiments demonstrate improved learning performance of our method, versus the sensitivity method. Our privacy-preserving technique does not depend on the sensitivity of the function, and extends easily to a class of convex loss functions. Our work also reveals an interesting connection between regularization and privacy.},
author = {Chaudhuri, Kamalika and Monteleoni, Claire},
doi = {10.12720/jait.6.3.88-95},
isbn = {9781605609492},
issn = {17982340},
journal = {Advances in Neural Information Processing Systems 21 - Proceedings of the 2008 Conference},
pages = {289--296},
title = {{Privacy-preserving logistic regression}},
year = {2009}
}
@article{Jayaraman2019,
abstract = {Differential privacy is a strong notion for privacy that can be used to prove formal guarantees, in terms of a privacy budget, $\epsilon$, about how much information is leaked by a mechanism. When used in privacy-preserving machine learning, the goal is typically to limit what can be inferred from the model about individual training records. However, the calibration of the privacy budget is not well understood. Implementations of privacy-preserving machine learning often select large values of $\epsilon$ in order to get acceptable utility of the model, with little understanding of the impact of such choices on meaningful privacy. Moreover, in scenarios where iterative learning procedures are used, relaxed definitions of differential privacy are often used which appear to reduce the needed privacy budget but present poorly understood trade-offs between privacy and utility. In this paper, we quantify the impact of these choices on privacy in experiments with logistic regression and neural network models. Our main finding is that there is no way to obtain privacy for free-relaxed definitions of differential privacy that reduce the amount of noise needed to improve utility also increase the measured privacy leakage. Current mechanisms for differentially private machine learning rarely offer acceptable utility-privacy trade-offs for complex learning tasks: settings that provide limited accuracy loss provide little effective privacy, and settings that provide strong privacy result in useless models.},
archivePrefix = {arXiv},
arxivId = {1902.08874},
author = {Jayaraman, Bargav and Evans, David},
eprint = {1902.08874},
isbn = {9781939133069},
journal = {Proceedings of the 28th USENIX Security Symposium},
pages = {1895--1912},
title = {{Evaluating differentially private machine learning in practice}},
year = {2019}
}
@article{Brown2024,
abstract = {We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss. Under modest assumptions on the input, we characterize the distribution of the iterate at each time step. Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data. This matches the dimension-dependence of the (nonprivate) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023). Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set. We validate our theorems through experiments on synthetic data.},
author = {Brown, Gavin and Dvijotham, Krishnamurthy and Evans, Georgina and Liu, Daogao and Smith, Adam and Thakurta, Abhradeep},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
pages = {4561--4584},
title = {{Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation}},
volume = {235},
year = {2024}
}
@article{Goldt2021,
abstract = {Understanding the impact of data structure on the computational tractability of learning is a key challenge for the theory of neural networks. Many theoretical works do not explicitly model training data, or assume that inputs are drawn component-wise independently from some simple probability distribution. Here, we go beyond this simple paradigm by studying the performance of neural networks trained on data drawn from pre-trained generative models. This is possible due to a Gaussian equivalence stating that the key metrics of interest, such as the training and test errors, can be fully captured by an appropriately chosen Gaussian model. We provide three strands of rigorous, analytical and numerical evidence corroborating this equivalence. First, we establish rigorous conditions for the Gaussian equivalence to hold in the case of single-layer generative models, as well as deterministic rates for convergence in distribution. Second, we leverage this equivalence to derive a closed set of equations describing the generalisation performance of two widely studied machine learning problems: two-layer neural networks trained using one-pass stochastic gradient descent, and full-batch pre-learned features or kernel methods. Finally, we perform experiments demonstrating how our theory applies to deep, pre-trained generative models. These results open a viable path to the theoretical study of machine learning models with realistic data.},
archivePrefix = {arXiv},
arxivId = {2006.14709},
author = {Goldt, Sebastian and Loureiro, Bruno and Reeves, Galen and Krzakala, Florent and M{\'{e}}zard, Marc and Zdeborov{\'{a}}, Lenka},
eprint = {2006.14709},
issn = {26403498},
journal = {Proceedings of Machine Learning Research},
keywords = {Generative models,Neural networks,Random Features,Stochastic Gradient Descent},
pages = {426--471},
title = {{The Gaussian equivalence of generative models for learning with shallow neural networks}},
url = {https://arxiv.org/abs/2006.14709v3},
volume = {145},
year = {2021}
}
@article{Chaudhuri2011,
abstract = {Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the $\epsilon$-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy- preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance. {\textcopyright} 2011 Kamalika Chaudhuri, Claire Monteleoni and Anand D. Sarwate.},
archivePrefix = {arXiv},
arxivId = {0912.0071},
author = {Chaudhuri, Kamalika and Monteleoni, Claire and Sarwate, Anand D.},
eprint = {0912.0071},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Classification,Empirical risk minimization,Logistic regression,Optimization,Privacy,Support vector machines},
pages = {1069--1109},
pmid = {21892342},
title = {{Differentially private empirical risk minimization}},
volume = {12},
year = {2011}
}
@article{Ganju2018,
abstract = {With the growing adoption of machine learning, sharing of learned models is becoming popular. However, in addition to the prediction properties the model producer aims to share, there is also a risk that the model consumer can infer other properties of the training data the model producer did not intend to share. In this paper, we focus on the inference of global properties of the training data, such as the environment in which the data was produced, or the fraction of the data that comes from a certain class, as applied to white-box Fully Connected Neural Networks (FCNNs). Because of their complexity and inscrutability, FCNNs have a particularly high risk of leaking unexpected information about their training sets; at the same time, this complexity makes extracting this information challenging. We develop techniques that reduce this complexity by noting that FCNNs are invariant under permutation of nodes in each layer. We develop our techniques using representations that capture this invariance and simplify the information extraction task. We evaluate our techniques on several synthetic and standard benchmark datasets and show that they are very effective at inferring various data properties. We also perform two case studies to demonstrate the impact of our attack. In the first case study we show that a classifier that recognizes smiling faces also leaks information about the relative attractiveness of the individuals in its training set. In the second case study we show that a classifier that recognizes Bitcoin mining from performance counters also leaks information about whether the classifier was trained on logs from machines that were patched for the Meltdown and Spectre attacks.},
author = {Ganju, Karan and Wang, Qi and Yang, Wei and Gunter, Carl A. and Borisov, Nikita},
doi = {10.1145/3243734.3243834},
isbn = {9781450356930},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
keywords = {Neural networks,Permutation equivalence,Property inference},
pages = {619--633},
title = {{Property inference attacks on fully connected neural networks using permutation invariant representations}},
year = {2018}
}
@article{Bombari2025,
abstract = {Differentially private gradient descent (DP-GD) is a popular algorithm to train deep learning models with provable guarantees on the privacy of the training data. In the last decade, theproblemofunderstandingitsperformancecostwithrespecttostandardGD has received remarkableattention fromtheresearchcommunity,whichhasledtoupper bounds on the excess population risk RP in different learning settings. However, such bounds typically degrade with overparameterization, i.e., as the number of parameters p gets larger than the number of training samples n—a regime which is ubiquitous in current deep-learning practice. As a result, the lack of theoretical insights leaves practitioners without clear guidance, leading some to reduce the effective number of trainable parameters to improve performance, while others use larger models to achieve better results through scale. In this work, we show that in the popular random features model with quadratic loss, for any sufficiently large p, privacy can be obtained for free, i.e., RP = o(1), not only when the privacy parameter has constant order but also in the strongly private setting = o(1). This challenges the common wisdom that overparameterization inherently hinders performance in private learning.},
author = {Bombari, Simone and Mondelli, Marco},
doi = {10.1073/pnas.2423072122},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {deep learning,differential privacy,differentially private gradient descent,overparameterization,random features model},
number = {15},
pmid = {40215275},
title = {{Privacy for free in the overparameterized regime}},
volume = {122},
year = {2025}
}
@article{Shokri2017,
abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial 'machine learning as a service' providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.},
archivePrefix = {arXiv},
arxivId = {1610.05820},
author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
doi = {10.1109/SP.2017.41},
eprint = {1610.05820},
isbn = {9781509055326},
issn = {10816011},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
pages = {3--18},
title = {{Membership Inference Attacks Against Machine Learning Models}},
year = {2017}
}
@article{Livni2014,
abstract = {It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.},
archivePrefix = {arXiv},
arxivId = {1410.1141},
author = {Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
eprint = {1410.1141},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {855--863},
title = {{On the computational efficiency of training neural networks}},
volume = {1},
year = {2014}
}
